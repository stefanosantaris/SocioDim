{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and external notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /media/santaris/HighSpeedStorage/LabsWorkspace/Notebooks/LocalityGroups/Graph Representation Learning/Matrix Factorization/SocioDim/datasets/Downloader.ipynb\n",
      "Version:  2.0.0\n",
      "Eager mode:  True\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from datasets.Downloader import *\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from numpy import linalg as LA\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from IPython import display\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of file: 976987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "datasets/BlogCatalog-dataset.zip: 100%|##########| 954k/954k [00:05<00:00, 193kB/s]  \n"
     ]
    }
   ],
   "source": [
    "download_dataset('http://socialcomputing.asu.edu/uploads/1283153973/BlogCatalog-dataset.zip', 'datasets/BlogCatalog-dataset.zip', 'datasets')\n",
    "# download_dataset('http://socialcomputing.asu.edu/uploads/1283157931/Flickr-dataset.zip', 'datasets/Flickr-dataset.zip', 'datasets')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_edgelist('datasets/BlogCatalog-dataset/data/edges.csv', delimiter=',', nodetype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes 10312\n",
      "Number of edges 333983\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of nodes {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = nx.adjacency_matrix(G)\n",
    "dense_matrix = np.array(adj_matrix.toarray(),dtype=np.float64)\n",
    "adj_tensor = tf.constant(dense_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('datasets/BlogCatalog-dataset/data/group-edges.csv', sep=',', names=['Node','Group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Node</th>\n",
       "      <th>Group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Node  Group\n",
       "0    28      1\n",
       "1    32      1\n",
       "2    36      1\n",
       "3    37      1\n",
       "4    84      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_labels = np.zeros((G.number_of_nodes(), 39), dtype=int)\n",
    "for index, row in labels.iterrows():\n",
    "    node = row['Node']\n",
    "    group = int(row['Group'])\n",
    "    vectorized_labels[node - 1][group - 1] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Extract Latent social dimensions based on network connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def modularity(adj_matrix):\n",
    "    \n",
    "    # Degrees of nodes\n",
    "    d = tf.reduce_sum(adj_matrix, 1)\n",
    "    \n",
    "    two_m = tf.reduce_sum(d)\n",
    "    \n",
    "    nom = tf.math.multiply(tf.transpose(d), d)\n",
    "    \n",
    "    B = tf.math.subtract(adj_matrix, tf.math.divide(nom, two_m))\n",
    "    \n",
    "    e,v = tf.linalg.eigh(B)\n",
    "        \n",
    "    return e,v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, v = modularity(adj_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigs_sort = tf.argsort(w, direction='DESCENDING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_latent = []\n",
    "for i in range(500):\n",
    "    v_latent.append(v[:][eigs_sort[i]].numpy())\n",
    "v_latent = np.array(np.transpose(v_latent), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 Construct Discriminative Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_Train_Test_Sets(ratio=0.1):\n",
    "    indexes = np.arange(G.number_of_nodes())\n",
    "    np.random.shuffle(indexes)\n",
    "    training_indexes = math.floor(G.number_of_nodes() * ratio)\n",
    "    training_nodes = indexes[:training_indexes]\n",
    "    testing_nodes = indexes[training_indexes+1:]\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    for node in training_nodes:\n",
    "        X.append(v_latent[node - 1])\n",
    "        Y.append(vectorized_labels[node - 1])\n",
    "\n",
    "\n",
    "    X_train = np.array(X)\n",
    "    X_train_tensor = tf.constant(X_train)\n",
    "    Y_train = np.array(Y)\n",
    "    Y_train_tensor = tf.constant(Y_train)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for node in testing_nodes:\n",
    "        X.append(v_latent[node - 1].real)\n",
    "        Y.append(vectorized_labels[node - 1])\n",
    "    X_train.shape\n",
    "\n",
    "    X_test = np.array(X)\n",
    "    X_test_tensor = tf.constant(X_test)\n",
    "    Y_test = np.array(Y)\n",
    "    Y_test_tensor = tf.constant(Y_test)\n",
    "    \n",
    "    return X_train_tensor, Y_train_tensor, X_test_tensor, Y_test_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructModel():\n",
    "    model = tf.keras.Sequential()\n",
    "    # Adds a densely-connected layer with 64 units to the model:\n",
    "    model.add(layers.Dense(1000, activation='relu'))\n",
    "    \n",
    "    model.add(layers.Dense(1000, activation='relu'))\n",
    "\n",
    "    # Add a sigmoid layer with 10 output units:\n",
    "    model.add(layers.Dense(39, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(1e-4)))\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio 0.1: Micro-F1 (%) 0.055673837661743164: Macro-F1 (%) 0.055673837661743164\n",
      "Ratio 0.2: Micro-F1 (%) 0.03856603056192398: Macro-F1 (%) 0.03856603056192398\n",
      "Ratio 0.30000000000000004: Micro-F1 (%) 0.0350453145802021: Macro-F1 (%) 0.0350453145802021\n",
      "Ratio 0.4: Micro-F1 (%) 0.042208634316921234: Macro-F1 (%) 0.042208634316921234\n",
      "Ratio 0.5: Micro-F1 (%) 0.04339886084198952: Macro-F1 (%) 0.04339886084198952\n",
      "Ratio 0.6: Micro-F1 (%) 0.03641679883003235: Macro-F1 (%) 0.03641679883003235\n",
      "Ratio 0.7000000000000001: Micro-F1 (%) 0.039753735065460205: Macro-F1 (%) 0.039753735065460205\n",
      "Ratio 0.8: Micro-F1 (%) 0.04753199219703674: Macro-F1 (%) 0.04753199219703674\n",
      "Ratio 0.9: Micro-F1 (%) 0.03729355335235596: Macro-F1 (%) 0.03729355335235596\n"
     ]
    }
   ],
   "source": [
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "for ratio in np.arange(0.1, 1, 0.1):\n",
    "    \n",
    "    X_train, Y_train, X_test, Y_test = construct_Train_Test_Sets(ratio)\n",
    "    \n",
    "    model = constructModel()\n",
    "    \n",
    "    model.fit(X_train, Y_train, epochs=100, batch_size=64, verbose=0)\n",
    "    \n",
    "    predict = model.predict(X_test)\n",
    "    predic_tensor = tf.constant(predict)\n",
    "    condition = tf.math.greater_equal(predic_tensor, 0.5)\n",
    "    bool_predictions = tf.where(condition, 1, 0)\n",
    "    \n",
    "    output_macro = tfa.metrics.F1Score(num_classes=39, average='macro')\n",
    "    output_macro.update_state(Y_test, bool_predictions)\n",
    "    \n",
    "    macro_f1.append(output_macro.result().numpy())\n",
    "    \n",
    "    output_micro = tfa.metrics.F1Score(num_classes=39, average='micro')\n",
    "    output_micro.update_state(Y_test, bool_predictions)\n",
    "    \n",
    "    micro_f1.append(output_micro.result().numpy())\n",
    "    \n",
    "    print(\"Ratio {}: Micro-F1 (%) {}: Macro-F1 (%) {}\".format(ratio, output_micro.result().numpy(), output_micro.result().numpy()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
